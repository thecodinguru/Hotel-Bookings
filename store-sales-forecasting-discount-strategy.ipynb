{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#! pip install lale\n#! pip install lime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Store Sales Forecasting & Discount Strategy:  \n\n**Goal:** \n* Exploratory Data Analysis to describe and clean the data, and to understand attributes\n* Feature selection to keep only important attributes\n* Developing a framework to evaluate and spot-check algorithms\n* Predicting and explaining future sales\n* Identifying the right time for discount strategies\n\n**Data set description:**\n\n* stores.csv\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of the store.\n\n* train.csv\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:  \n\nStore - the store number  \nDept - the department number   \nDate - the week \nWeekly_Sales -  sales for the given department in the given store  \nIsHoliday - whether the week is a special holiday week  \n\n* test.csv\n\nThis file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.  \n\n* features.csv\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:  \n\nStore - the store number  \nDate - the week  \nTemperature - the average temperature in the region  \nFuel_Price - the cost of fuel in the region  \nMarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.  \nCPI - the consumer price index   \nUnemployment - the unemployment rate   \nIsHoliday - whether the week is a special holiday week  "},{"metadata":{},"cell_type":"markdown","source":"> # 1. Prepare Problem\n> # 1.a) Load libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nimport IPython\nimport pandas as pd\nfrom pandas import set_option\nset_option('display.width', 160)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # 1.b) Load dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/new-walmart-data/train.csv\", parse_dates=[\"Date\"])\ntest=pd.read_csv(\"/kaggle/input/new-walmart-data/test.csv\", parse_dates=[\"Date\"])\nstores=pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\nfeatures = pd.read_csv(\"/kaggle/input/new-walmart-data/features.csv\", parse_dates=[\"Date\"])\n# merging attributes into one dataset\ntrain = train.merge(stores, how='left').merge(features, how='left')\ntest = test.merge(stores, how='left').merge(features, how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data analysis :**\n\nWhy? It is important to know your data, extract the maximum information from it and gather as many insights from it, to get the best results. We will summarize the data and try to understand the relationships in it, by using statistical tools.  "},{"metadata":{},"cell_type":"markdown","source":"> # 2. Summarize Data  \n> # 2.a) Cleaning data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the test dataset don't contain the features included in the train dataset, taking into consideration that these features (Temperature, Fuel price, MarkDowns, CPI and Unemployment) cannot be used in the test dataset due to their high dependences on the date, so it will be a good idea to delete them. but before that, we will make sure that these features don't provide any information on the target 'Weekly_Sales'. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values\ntrain.isnull().mean()*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MarkDowns attributes have more than 64% of null values, in addition, the lack of their meaning make it difficult to fill the columns with appropriate values. \nLet's check their correlations with the target 'Weekly_Sales'"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[['Weekly_Sales','Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n                 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']].corr()\nfig, ax = plt.subplots(figsize=(18, 12))\nax = sns.heatmap(corr , vmin=-1, vmax=1, annot=True, fmt='.2f', cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abs(corr[\"Weekly_Sales\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlations of these features with the target 'WeeklySales' are approximately 0. So it's save to delete them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n                    'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment'], axis = 1)\ntest = test.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n                    'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment'], axis = 1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another useful step is to facilate the acces to the 'Date' attribute by splitting it into its componenents (i.e. Year, Month and week)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['week']=train['Date'].dt.week\ntrain['year']=train['Date'].dt.year\ntest['week']=test['Date'].dt.week\ntest['year']=test['Date'].dt.year\ndel test['Date']\ndel train['Date']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last step is to treat the categorigal attributes, namely, Type and IsHoliday:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's count and see the distinct value of Type :  \nprint(train[['Type', 'IsHoliday']].nunique())\nTypes = train['Type'].unique()\nHoli = train['IsHoliday'].unique()\nprint(f'The 3 types of Store : {Types}')\nprint(f'The Holiday Flag : {Holi}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntrain['Type']=LabelEncoder().fit_transform(train['Type'])\ntrain['IsHoliday']=LabelEncoder().fit_transform(train['IsHoliday'])\nTypes = train['Type'].unique()\nHoli = train['IsHoliday'].unique()\nprint(f'The new 3 types of Store : {Types}')\nprint(f'The Holiday Flag : {Holi}')\n\n# Test set\ntest['IsHoliday']=LabelEncoder().fit_transform(test['IsHoliday'])\ntest['Type']=LabelEncoder().fit_transform(test['Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we proceed to analyse each attribute."},{"metadata":{},"cell_type":"markdown","source":"> # 2.b) Descriptive statistics & data visualizations:\n# Weekly_Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weekly_Sales statistics:\ntrain['Weekly_Sales'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weekly_Sales mean is much higher than the median (50%), which make its distribution skewed to the right."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(train['Weekly_Sales'], shade=True);\nplt.show()\nprint(\"%s: mean (%f), variance (%f), skewness (%f), kurtosis (%f)\" \n      % ('Weekly_Sales', np.mean(train['Weekly_Sales']), np.var(train['Weekly_Sales']),\n         skew(train['Weekly_Sales']), kurtosis(train['Weekly_Sales'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot makes the right skewness clear, so most weeks have sales around the median.  \nAlso, we can see that the Weekly_Sales attribute has a large kurtosis which indicates the presence of extreme values, in other words, some weeks have high sales. It would be a good idea to know the origins of these extreme values. "},{"metadata":{},"cell_type":"markdown","source":"We can have some idea on attributes contributing to the target Weekly_Sales, by calculating correlations : "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weekly_Sales Correlations:\ncor_Sales = abs(train.corr()['Weekly_Sales']).sort_values(ascending=False)\nprint(cor_Sales, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* At first glance, the characteristics of the store (Type, Size and Departments) appear to have some information on the target."},{"metadata":{},"cell_type":"markdown","source":"# Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Type Correlations:\ncor_Type = abs(train.corr()['Type']).sort_values(ascending=False)\nprint(cor_Type, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The store Type is highly correlated with its Size."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The portion of each type of store:\nprint(train.groupby('Type').size())\nOutLabels = [str(train['Type'].unique()[i]) for i in range(train['Type'].nunique())]\nOutValues = [train['Type'].value_counts()[i] for i in range(train['Type'].nunique())]\npie=go.Pie(labels=OutLabels,values=OutValues)\npy.iplot([pie])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear from the Pie chart that Type 0 is the main type with more than half stores, followed by Type 1 store, then Type 2.\n\nLet's investigate the relation beetwen the store Type and other attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def xyBox(X,Y,showfliers=True, hue=None, figsize=(9, 6)) :\n    f, ax = plt.subplots(figsize=figsize)\n    fig = sns.boxplot(x=X, y=Y, data=train, showfliers=showfliers,hue=hue, showmeans=True, meanline=True, meanprops = dict(linestyle='--', linewidth=2.5, color='red'))\n    fig.text(-1.5, 0, 'mean : ---', color='red', weight='roman',size=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xyBox('Type', 'Size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By neglecting the outliers, we can say that the store Type determines the size of it, Type 0 has the highest size and Type 2 has the lowest.  \n-> The two attributes are highly correlated (~0.81)"},{"metadata":{"trusted":true},"cell_type":"code","source":"xyBox('Type', 'Store')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Type 0 includes a large range of possible stores, which overlaps with other Types. As a result, we have a moderate correlation (~0.22) between Type and Store.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"xyBox('Type', 'Weekly_Sales',showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that store Type 0 has the highest median (mean) in terms of weekly sales, and Type 2 has the lowest.  \n\nOtherwise, the range of Weekly_Sales overlaps in the 3 Types, which reduces the correlation value to ~0.18.    \n\n-> The store Type may provide information on the target."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n* The store Type may provide information on the target.  \n* The store Type give implicitly information on the size of the store.\n* Feature selection method to keep either the Size or the Type, later on."},{"metadata":{},"cell_type":"markdown","source":"# Size"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size Correlations:\ncor_Size = abs(train.corr()['Size']).sort_values(ascending=False)\nprint(cor_Size, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size statistics:\ntrain['Size'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size distribution :\nsns.distplot(train['Size']);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The size has a balanced distribution (mean ~ median), and a large range.\n\nRoughly speaking, the store Size distribution has 3 \"peaks\", which can be attributed to the store Type as seen before.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size relation with Weekly_Sales\nsns.jointplot(x='Size', y='Weekly_Sales', data=train, height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is some random high sale values appear as the size increases, which corresponds to the moderate correlation between the Size and Weekly_Sales  (~0.24)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size relation with Store\nsns.jointplot(x='Size', y='Store', data=train, height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't recognize any pattern between the Size and Stores, which explains the poor correlation of ~0.18."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n* The Size may provide information on the target.  \n* The Size is not correlated with other attributes, except the Type."},{"metadata":{},"cell_type":"markdown","source":"# Store"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store Correlations:\ncor_Store = abs(train.corr()['Store']).sort_values(ascending=False)\nprint(cor_Store, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is only small correlations between Store and Type, Size."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18, 12))\nfig = sns.boxplot(x='Store', y='Weekly_Sales', data=train, showfliers=False, showmeans=True, meanline=True, meanprops = dict(linestyle='--', linewidth=2.5, color='red'))\nfig.text(-8, 0, 'mean : ---', color='red', weight='roman',size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales are different for each store, so they may depend on the Store."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n* The Store may provide some information on Sales."},{"metadata":{},"cell_type":"markdown","source":"# Dept"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dept Correlations:\ncor_Dept = abs(train.corr()['Dept']).sort_values(ascending=False)\nprint(cor_Dept, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Dept is moderately correlated with Weekly_Sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dept relation with Weekly_Sales\nfig, ax = plt.subplots(figsize=(18, 12))\nfig = sns.boxplot(x='Dept', y='Weekly_Sales', data=train, showfliers=False, showmeans=True, meanline=True, meanprops = dict(linestyle='--', linewidth=2.5, color='red'))\nfig.text(-8, 0, 'mean : ---', color='red', weight='roman',size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Departements show distinct range of Sales, which may tell something about them."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**  \n* Each Departement may contain some information on the target. "},{"metadata":{},"cell_type":"markdown","source":"# Week"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Week Correlations:\ncor_week = abs(train.corr()['week']).sort_values(ascending=False)\nprint(cor_week, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no correlation between the week and Sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of sales over weeks:\nfig, ax = plt.subplots(figsize=(18, 12))\npalette = sns.color_palette(\"mako_r\", 3)\nfig = sns.lineplot(x='week', y='Weekly_Sales', data=train, hue='year', err_style=None, palette=palette)\nplt.xticks(np.arange(1, 53, step=1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of Sales over weeks is not impacted by the year, so the year is irrelevant to this situation.  \n\nThe Holidays by week:    \nWeek 6  : Super Bowl  \nWeek 36 : Labor Day  \nWeek 47 : Thanksgiving  \nWeek 52 : Christmas   \nThis explains peaks in Sales at these weeks. However there is a subtlety concerning the Christmas, the peak in Sales happens at the Week 51, one week before the Christmas week, this can be explained by the fact that people prepare for the Christmas days before, so shifting the Holiday flag for Christmas to Week 51 will be a good choice.  \n\nAnother high sales correspond to Holidays omitted from the train set, namely :  \nEaster Day       : Week 13(2010), Week 16(2011), Week 14(2012), Week 13(2013) for test set  \nMemorial Day     : Week 22  \nIndependence Day : Week 27  \n\nChanging IsHoliday Flag to '1' in these Weeks, would be a good idea."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Easter Day\ntrain.loc[(train.year==2010) & (train.week==13), 'IsHoliday'] = 1\ntrain.loc[(train.year==2011) & (train.week==16), 'IsHoliday'] = 1\ntrain.loc[(train.year==2012) & (train.week==14), 'IsHoliday'] = 1\ntest.loc[(test.year==2013) & (test.week==13), 'IsHoliday'] = 1\n# Memorial & Independence Day\ntrain.loc[((train.week==22) | (train.week==27)), 'IsHoliday'] = 1\ntest.loc[((test.week==22) | (test.week==27)), 'IsHoliday'] = 1\n# shifting the Christmas Week\ntrain.loc[(train.week==52), 'IsHoliday'] = 0\ntrain.loc[(train.week==51), 'IsHoliday'] = 1\ntest.loc[(test.week==52), 'IsHoliday'] = 0\ntest.loc[(test.week==51), 'IsHoliday'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n* The week doesn't tell much about Sales, except for weeks marked as HoliDays."},{"metadata":{},"cell_type":"markdown","source":"# HoliDays"},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the impact of Holidays on Sales:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 8))\nfig = sns.stripplot(x='IsHoliday', y='Weekly_Sales', data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, Holiday Sales have some high values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weekly Sales per Dept with the Holiday Flag:\nfig, ax = plt.subplots(figsize=(18, 12))\nfig = sns.boxplot(x='Store', y='Weekly_Sales', data=train, hue='IsHoliday', showfliers=False)\n# Weekly Sales per Store with the Holiday Flag:\nfig, ax = plt.subplots(figsize=(18, 12))\nfig = sns.boxplot(x='Dept', y='Weekly_Sales', data=train, hue='IsHoliday', showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, Sales in Stores and Department increase slightly during Holidays, except for some small number of Departments, for instance, Weekly Sales median of Dept 38 decreases due to low sales during Holidays, maybe because of the nature of products in these Departments.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weekly Sales per store Type with the Holiday Flag:\nxyBox('Type','Weekly_Sales', hue='IsHoliday', showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same result, Weekly Sales increase slightly at Holidays."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n* Sales increase at Holidays."},{"metadata":{},"cell_type":"markdown","source":"> # 3) Feature selection :  \n\nWhy?\n\n\nAfter analyzing and knowing the data, it's time to keep only important features, because irrelevant or redundant features impact negatively the accuracy of algorithms and cause overfitting problems. Moreover, reducing features enable the algorithm to train faster and make the model easier to interpret."},{"metadata":{},"cell_type":"markdown","source":"First, let's see all features in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attributes:\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second, let's check features importance using the random forest algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating the target from data:\nX = train.drop('Weekly_Sales',axis=1)\ny_train = train['Weekly_Sales']\n\n# importing the random forest algorithm\nfrom sklearn.ensemble import RandomForestRegressor\n# fitting the model\nrf = RandomForestRegressor()\nrf.fit(X,y_train)\n# Feature importance:\npd.DataFrame({'Features':X.columns,'Relative Importance':rf.feature_importances_}).sort_values(by='Relative Importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the most important feature is Dept, followed by Size.  \nAs seen before, Type and Size are highly correlated and contain the same information, now we can decide what feature to keep, in this case 'Size'.  \nAs discussed previously, 'year' is irrelevant and don't contain any information on the target -> Drop it from data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X.drop(['Type', 'year'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) Evaluate Algorithms"},{"metadata":{},"cell_type":"markdown","source":"After analysing, cleaning and preparing the data, the next step is to select the best algorithm with the optimal parameters to obtain the best results.  \nThis step requiers manually selecting the type of data normalization, manually selecting algorithms and tune all hyperparameters, which is a complex task to do. Instead, we will use Auto-ML, which is a series of concepts and techniques used to automate these processes and help reducing bias and errors.  \nAuto-ML can be done using Lale, an easy-to-use library and a powerful tool wich serves for hyperparameter tuning and algorithm selection. "},{"metadata":{},"cell_type":"markdown","source":"Many algorithms assume normal distribution of the data, especially when features have different ranges like our case, so it is necessary to implement this step in our pipeline.  \n\nFor data normalization, Lale will have the following choices :  \n\n* MinMaxscaler: scaling data values in the range [0,1]\n* StandardScaler: data distribution will have a mean = 0 and std =1\n* PCA: for linear dimensionality reduction\n* NoOp: keep the data unchanged  \n\nAlgorithms used for spot-checking :  \n\n* LinearRegression\n* RandomForestRegressor\n* GradientBoostingRegressor\n* ExtraTreesRegressor\n* KNeighborsRegressor\n* SVR\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lale\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\nfrom lale.lib.lale import Hyperopt, NoOp, GridSearchCV\nfrom lale.pretty_print import ipython_display\nimport lale.schemas as schemas\nimport lale.helpers\nlale.wrap_imported_operators()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For hyperparameter tuning, Lale give us the choice to use its search space or schemas as is, or we can customize the schemas to fit our purposes (e.g. reduce the search space to speed up the search).  \nLet's see Lale schemas for the following algorithms and custumize it to reduce the training time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestRegressor hyperparameter to costumize:\nprint( 'RandomForestRegressor:\\n')\nipython_display(RandomForestRegressor.hyperparam_schema('n_estimators'))\nipython_display(RandomForestRegressor.hyperparam_schema('min_samples_leaf'))\n# ExtraTreesRegressor hyperparameter to costumize:\nprint( 'ExtraTreesRegressor:\\n')\nipython_display(ExtraTreesRegressor.hyperparam_schema('n_estimators'))\nipython_display(ExtraTreesRegressor.hyperparam_schema('min_samples_leaf'))\n# GradientBoostingRegressor hyperparameter to costumize:\nprint( 'GradientBoostingRegressor:\\n')\nipython_display(GradientBoostingRegressor.hyperparam_schema('n_estimators'))\nipython_display(GradientBoostingRegressor.hyperparam_schema('min_samples_leaf'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the search space for number of trees in the forest is in the range [10, 100], for min_samples_leaf is in the range [1, 20] for all algorithms, so we will reduce them to [10, 20] and [1, 5] respectively. "},{"metadata":{"trusted":true},"cell_type":"code","source":"RandomForestRegressor = RandomForestRegressor.customize_schema(n_estimators=schemas.Int(min=10, max=20),\n                                                               min_samples_leaf=schemas.Int(min=1, max=5))\n\nExtraTreesRegressor = ExtraTreesRegressor.customize_schema(n_estimators=schemas.Int(min=10, max=20),\n                                                           min_samples_leaf=schemas.Int(min=1, max=5))\n\nGradientBoostingRegressor = GradientBoostingRegressor.customize_schema(n_estimators=schemas.Int(min=10, max=20),\n                                                                       min_samples_leaf=schemas.Int(min=1, max=5))                                                           ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will create our pipeline and visualize it easily with Lale."},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = (StandardScaler | MinMaxScaler | PCA | NoOp)  >>  (LinearRegression | KNeighborsRegressor |\n               RandomForestRegressor | GradientBoostingRegressor | ExtraTreesRegressor  | SVR )\npipeline.visualize()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We choose the negative mean absolute error metric (which is easier to understand and to interpret) to define the loss function, and let Hyperopt determine the optimal pipeline that minimize the loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'neg_mean_absolute_error'\npip_selection =  Hyperopt(estimator = pipeline, cv = 3, max_evals = 20, scoring=scoring, max_eval_time=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip_trained = pip_selection.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the best pipeline with optimal parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip_trained.get_pipeline().visualize()\npip_trained.get_pipeline().pretty_print(ipython_display= True, show_imports= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best pipeline don't use any data normalization, which is easy to understand since the selected ExtraTreesRegressor algorithm is not affected by any data transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of all pipelines:\npip_trained.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best = pip_trained.summary().loss.argmin()\nloss_b = pip_trained.summary().loss.min()\nworst = pip_trained.summary().loss.argmax()\nloss_w = pip_trained.summary().loss.max()\nprint(f'The best pipeline is {best} with a loss of {loss_b}')\nprint(f'The worst pipeline is {worst} with a loss of {loss_w}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Finalize Model : Predictions on test dataset"},{"metadata":{},"cell_type":"markdown","source":"Now, we train the model on the entire training dataset using ExtraTreesRegressor algorithm with optimal parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_trees_regressor = ExtraTreesRegressor(min_samples_leaf=3, min_samples_split=14, n_estimators=15)\nextra_trees_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing the test dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test.drop(['Type', 'year'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make and evaluate predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = extra_trees_regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After making predictions, many questions one can ask, can we trust certain predictions and take actions based on them? \nThe answer is provided by LIME, hence, we will use its techniques to explain some predictions."},{"metadata":{},"cell_type":"markdown","source":"Firstly, we define the explainer provided by LIME"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lime\nimport lime.lime_tabular\n# Define the explainer:\nX_train_arr = X_train.to_numpy()\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train_arr, feature_names=X_train.columns, \n            class_names=['Weekly_Sales'], verbose=False, mode='regression',\n            categorical_features=['IsHoliday'], discretizer='decile', random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we select 2 instances, the first isn't a Holiday week, the second is, and we show explanations."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_arr = X_test.to_numpy()\ni = 1\nexp = explainer.explain_instance(X_test_arr[i], extra_trees_regressor.predict, num_features=5)\nexp.show_in_notebook(labels=None, predict_proba=True, show_predicted_value=True)\nprint(f'Document id: {i}')\nprint('Weekly Sales prediction:', extra_trees_regressor.predict(X_test_arr)[i])\nprint ('Explanation for prediction:')\nprint ('\\n'.join(map(str, exp.as_list())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 10055\nexp = explainer.explain_instance(X_test_arr[i], extra_trees_regressor.predict, num_features=5)\nexp.show_in_notebook(labels=None, predict_proba=True, show_predicted_value=True)\nprint(f'Document id: {i}')\nprint('Weekly Sales prediction:', extra_trees_regressor.predict(X_test_arr)[i])\nprint ('Explanation for prediction:')\nprint ('\\n'.join(map(str, exp.as_list())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see how each attribute contributes to the prediction, either positively or negatively.  \nThe Holiday Flag is 0 in the first instance, which affects negatively predicted Sales, on the other hand, the Dept 1 has relatively high Sales which positively impacts predicted Sales.  \nOtherwise, the large size of the second example contributes significantly to predicted Sales, in contrast to Dept 67, with its relatively low Sales, affects negatively predicted Sales.  \n-> we can say that these predictions are trustworthy."},{"metadata":{},"cell_type":"markdown","source":"Can we say now that we trust the model to exploit it in real life? To see if we can trust the model, a global explanation of it is required, hence, we will select a representative and non-redundant set of explanations that provide a global perspective on the model.  \nThis is done using SP-LIME which is an extension of LIME.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code for SP-LIME\nimport warnings\nfrom lime import submodular_pick\n\nsp_obj = submodular_pick.SubmodularPick(explainer, X_test_arr, extra_trees_regressor.predict, num_features=5,num_exps_desired=8)\n\n[exp.show_in_notebook() for exp in sp_obj.sp_explanations]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After analyzing these explanations we can say that they are trustworthy and the model can be trusted as well. "},{"metadata":{},"cell_type":"markdown","source":"Assuming the predictions are trustworthy, I will suggest some actions to take based on these predictions.  \n\nI will first plot Weekly Sales over Week, knowing that predictions begin at Week 44, 2012 and end at Week 30, 2013."},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data\npredictions = pd.DataFrame(predictions, columns=['Weekly_Sales'])\ntest_ = pd.concat([test.drop('Type', axis=1),predictions],axis=1)\n# training data\nWS_train = pd.DataFrame(train.Weekly_Sales, columns=['Weekly_Sales'])\ntrain_ = pd.concat([X.drop('Type', axis=1),WS_train],axis=1)\n# data\ndata = pd.concat([test_,train_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of sales over weeks:\nfig, ax = plt.subplots(figsize=(18, 12))\npalette = sns.color_palette(\"mako_r\", 4)\nfig = sns.lineplot(x='week', y='Weekly_Sales', data = data, hue='year', err_style=None, palette=palette)\nplt.xticks(np.arange(1, 53, step=1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the plot of predicted Sales has the same signature as the original Sales.  \nThe model predicts low Sales right after Easter Day, starting from Week 14, for that reason planning bundled and volume discounts during these Weeks may improve Sales. The same goes for Weeks before Thanksgiving Holiday.  Moreover, event discounts in some Weeks' Holiday wich have relatively low sales (SuperBowl, Easter Day, Memorial Day, Independence Day and Labor Day) compared to other Weeks' Holiday (Thanksgiving or Christmas), may potentially enhance Sales.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}